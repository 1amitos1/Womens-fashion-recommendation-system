{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam ,SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "import glob,matplotlib as plt\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input,decode_predictions\n",
    "from keras.applications.resnet50 import ResNet50,preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "\n",
    "def get_compile_model():\n",
    "    img_width, img_height = 224, 224\n",
    "    # model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "    model = applications.ResNet50(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    # Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "#     for layer in model.layers[:16]:\n",
    "#         layer.trainable = False\n",
    "#     for layer in model.layers[16:]:\n",
    "#         layer.trainable = True\n",
    "        \n",
    "#     for layer in model.layers:\n",
    "#         layer.trainable = False\n",
    "    # Adding custom Layers\n",
    "    x = model.output\n",
    "    #print(x.shape)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    ## 4 as the number of categories\n",
    "    predictions = Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "    # creating the final model\n",
    "    model_final = Model(input=model.input, output=predictions)\n",
    "    # model.load_weights('caavo-1.h5')\n",
    "    # print(model.summary())\n",
    "    adam = Adam(lr=1e-3)\n",
    "    model_final.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20934 files belonging to 4 classes.\n",
      "Blouses_Shirts :  0\n",
      "   Dresses :  1\n",
      "    Shorts :  2\n",
      "    Skirts :  3\n",
      "Found 7661 files belonging to 4 classes.\n",
      "Blouses_Shirts :  0\n",
      "   Dresses :  1\n",
      "    Shorts :  2\n",
      "    Skirts :  3\n",
      "Epoch 1/5\n",
      "210/210 [==============================] - 480s 2s/step - loss: 0.6764 - accuracy: 0.7285 - val_loss: 1.8730 - val_accuracy: 0.5169\n",
      "Epoch 2/5\n",
      "210/210 [==============================] - 451s 2s/step - loss: 0.4718 - accuracy: 0.8123 - val_loss: 0.5569 - val_accuracy: 0.7148\n",
      "Epoch 3/5\n",
      "210/210 [==============================] - 451s 2s/step - loss: 0.4152 - accuracy: 0.8322 - val_loss: 0.3968 - val_accuracy: 0.7710\n",
      "Epoch 4/5\n",
      "210/210 [==============================] - 451s 2s/step - loss: 0.3697 - accuracy: 0.8501 - val_loss: 1.1818 - val_accuracy: 0.7259\n",
      "Epoch 5/5\n",
      "210/210 [==============================] - 451s 2s/step - loss: 0.3215 - accuracy: 0.8691 - val_loss: 0.4634 - val_accuracy: 0.7495\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Data Generator inherited from keras.utils.Sequence\n",
    "    Args:\n",
    "        directory: the path of data set, and each sub-folder will be assigned to one class\n",
    "        batch_size: the number of data points in each batch\n",
    "        shuffle: whether to shuffle the data per epoch\n",
    "    Note:\n",
    "        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n",
    "        # Initialize the params\n",
    "        self.batch_size = batch_size\n",
    "        self.directory = directory\n",
    "        self.shuffle = shuffle\n",
    "        self.data_aug = data_augmentation\n",
    "        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n",
    "        self.X_path, self.Y_dict = self.search_data()\n",
    "        # Print basic statistics information\n",
    "        self.print_stats()\n",
    "\n",
    "\n",
    "    def search_data(self):\n",
    "        X_path = []\n",
    "        Y_dict = {}\n",
    "        # list all kinds of sub-folders\n",
    "        self.dirs = sorted(os.listdir(self.directory))\n",
    "        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n",
    "        for i, folder in enumerate(self.dirs):\n",
    "            folder_path = os.path.join(self.directory, folder)\n",
    "            for file in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                # append the each file path, and keep its label\n",
    "                X_path.append(file_path)\n",
    "                Y_dict[file_path] = one_hots[i]\n",
    "        return X_path, Y_dict\n",
    "\n",
    "    def print_stats(self):\n",
    "        # calculate basic information\n",
    "        self.n_files = len(self.X_path)\n",
    "        self.n_classes = len(self.dirs)\n",
    "        self.indexes = np.arange(len(self.X_path))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        # Output states\n",
    "        print(\"Found {} files belonging to {} classes.\".format(self.n_files, self.n_classes))\n",
    "        for i, label in enumerate(self.dirs):\n",
    "            print('%10s : ' % (label), i)\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        # calculate the iterations of each epoch\n",
    "        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n",
    "        return int(steps_per_epoch)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get the data of each batch\n",
    "        \"\"\"\n",
    "        # get the indexs of each batch\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # using batch_indexs to get path of current batch\n",
    "        batch_path = [self.X_path[k] for k in batch_indexs]\n",
    "        # get batch data\n",
    "        batch_x, batch_y = self.data_generation(batch_path)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle the data at each end of epoch\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def read_image(self,img_path, H, W):\n",
    "        #print(f\"in read img\\n{img_path}\\n\")\n",
    "        # img_path=img_path+\".jpg\"\n",
    "        #print(f\"in read img--{img_path}\")\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (W, H))  # you can resize to  (128,128) or (256,256)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def data_generation(self, batch_path):\n",
    "        #print(f\"in data generation\\n {batch_path}\\n\")\n",
    "        # load data into memory, you can change the np.load to any method you want\n",
    "        batch_x = [self.load_data(x) for x in batch_path]\n",
    "        batch_y = [self.Y_dict[x] for x in batch_path]\n",
    "        # transfer the data format and take one-hot coding for labels\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def load_data(self, path):\n",
    "        # load the processed .npy files which have 5 channels (1-3 for RGB, 4-5 for optical flows)\n",
    "        #data = np.load(path, mmap_mode='r')\n",
    "        #print(f\"[+][+] in load data:path-->{path}]n\")\n",
    "        data = self.read_image(path, 224, 224)\n",
    "        data = np.float32(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "#################################################################3\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "    #if epoch  == 1 and epoch != 0:\n",
    "    #print(\"---------------in scheduler-----------------------\")\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.7)\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)    \n",
    "\n",
    "model =get_compile_model()\n",
    "#from keras.models import load_model\n",
    "\n",
    "#path=r'/home/jupyter/training_data/weights/weights_at_epoch_1.h5'\n",
    "#model = model.load_weights(r'/home/jupyter/training_data/weights/weights_at_epoch_1.h5')\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "class MyCbk(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model):\n",
    "         self.model_to_save = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model_to_save.save(r'/home/jupyter/training_data/model/model_at_epoch_%d.h5' % (epoch+1))\n",
    "        self.model_to_save.save_weights(r'/home/jupyter/training_data/weights/weights_at_epoch_%d.h5' % (epoch+1))\n",
    "\n",
    "check_point = MyCbk(model)\n",
    "\n",
    "\n",
    "filename = r'/home/jupyter/training_data/log/ours_log.csv'\n",
    "csv_logger = CSVLogger(filename, separator=',', append=True)\n",
    "callbacks_list = [check_point, csv_logger, reduce_lr]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_train = r'/home/jupyter/Train'\n",
    "path_val = r'/home/jupyter/Val'\n",
    "num_epochs=5\n",
    "batch_size=100\n",
    "checkpoint_dir=\"\"\n",
    "num_workers=8\n",
    "\n",
    "train_generator = DataGenerator(directory=path_train,batch_size=batch_size,data_augmentation=False)\n",
    "\n",
    "val_generator = DataGenerator(directory=path_val, batch_size=batch_size,data_augmentation=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    epochs=num_epochs,\n",
    "    workers=num_workers ,\n",
    "    max_queue_size=4,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
