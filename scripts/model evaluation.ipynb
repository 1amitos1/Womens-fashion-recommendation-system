{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Data Generator inherited from keras.utils.Sequence\n",
    "    Args:\n",
    "        directory: the path of data set, and each sub-folder will be assigned to one class\n",
    "        batch_size: the number of data points in each batch\n",
    "        shuffle: whether to shuffle the data per epoch\n",
    "    Note:\n",
    "        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n",
    "        # Initialize the params\n",
    "        self.batch_size = batch_size\n",
    "        self.directory = directory\n",
    "        self.shuffle = shuffle\n",
    "        self.data_aug = data_augmentation\n",
    "        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n",
    "        self.X_path, self.Y_dict = self.search_data()\n",
    "        # Print basic statistics information\n",
    "        self.print_stats()\n",
    "\n",
    "\n",
    "    def search_data(self):\n",
    "        X_path = []\n",
    "        Y_dict = {}\n",
    "        # list all kinds of sub-folders\n",
    "        self.dirs = sorted(os.listdir(self.directory))\n",
    "        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n",
    "        for i, folder in enumerate(self.dirs):\n",
    "            folder_path = os.path.join(self.directory, folder)\n",
    "            for file in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                # append the each file path, and keep its label\n",
    "                X_path.append(file_path)\n",
    "                Y_dict[file_path] = one_hots[i]\n",
    "        return X_path, Y_dict\n",
    "\n",
    "    def print_stats(self):\n",
    "        # calculate basic information\n",
    "        self.n_files = len(self.X_path)\n",
    "        self.n_classes = len(self.dirs)\n",
    "        self.indexes = np.arange(len(self.X_path))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        # Output states\n",
    "        print(\"Found {} files belonging to {} classes.\".format(self.n_files, self.n_classes))\n",
    "        for i, label in enumerate(self.dirs):\n",
    "            print('%10s : ' % (label), i)\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        # calculate the iterations of each epoch\n",
    "        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n",
    "        return int(steps_per_epoch)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get the data of each batch\n",
    "        \"\"\"\n",
    "        # get the indexs of each batch\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # using batch_indexs to get path of current batch\n",
    "        batch_path = [self.X_path[k] for k in batch_indexs]\n",
    "        # get batch data\n",
    "        batch_x, batch_y = self.data_generation(batch_path)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle the data at each end of epoch\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def read_image(self,img_path, H, W):\n",
    "        #print(f\"in read img\\n{img_path}\\n\")\n",
    "        # img_path=img_path+\".jpg\"\n",
    "        #print(f\"in read img--{img_path}\")\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (W, H))  # you can resize to  (128,128) or (256,256)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def data_generation(self, batch_path):\n",
    "        #print(f\"in data generation\\n {batch_path}\\n\")\n",
    "        # load data into memory, you can change the np.load to any method you want\n",
    "        batch_x = [self.load_data(x) for x in batch_path]\n",
    "        batch_y = [self.Y_dict[x] for x in batch_path]\n",
    "        # transfer the data format and take one-hot coding for labels\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def load_data(self, path):\n",
    "        # load the processed .npy files which have 5 channels (1-3 for RGB, 4-5 for optical flows)\n",
    "        #data = np.load(path, mmap_mode='r')\n",
    "        #print(f\"[+][+] in load data:path-->{path}]n\")\n",
    "        data = self.read_image(path, 224, 224)\n",
    "        data = np.float32(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 6666 files belonging to 4 classes.\n",
      "Blouses_Shirts :  0\n",
      "   Dresses :  1\n",
      "    Shorts :  2\n",
      "    Skirts :  3\n",
      "Number of sample=6666\n",
      "\n",
      "[[2171  347  142  119]\n",
      " [ 251 1690   25  215]\n",
      " [  95   67  700  207]\n",
      " [  20   79   31  507]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B      0.856     0.781     0.817      2779\n",
      "           D      0.774     0.775     0.775      2181\n",
      "          SH      0.780     0.655     0.712      1069\n",
      "          SK      0.484     0.796     0.602       637\n",
      "\n",
      "    accuracy                          0.760      6666\n",
      "   macro avg      0.723     0.752     0.726      6666\n",
      "weighted avg      0.781     0.760     0.766      6666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.optimizers import adam, Adam\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def read_image(img_path,H=224,W=224):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (W,H)) # you can resize to  (128,128) or (256,256)\n",
    "    return img\n",
    "\n",
    "def load_model():\n",
    "    # load json and create model\n",
    "    json_file = open(r\"E:\\FINAL_PROJECT_DATA\\FashioNet\\Models\\model.json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_final = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_final.load_weights(r\"E:\\FINAL_PROJECT_DATA\\FashioNet\\Models\\save_weights.h5\")\n",
    "    adam = Adam(lr=1e-3)\n",
    "    model_final.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model_final\n",
    "\n",
    "def get_y_true_label(y_true_vec):\n",
    "    pre_dict = {0: \"Blouses\", 1: \"Dresses\", 2: \"Shorts\", 3: \"Skirts\"}\n",
    "    pre_dict2 = {\"Blouses\":'B',\"Dresses\":'D' ,\"Shorts\":'SH',\"Skirts\":'SK'}\n",
    "    # Get the maximum element from a Numpy array\n",
    "    maxElement = np.amax(y_true_vec)\n",
    "    #print('Max element from Numpy Array : ', maxElement)\n",
    "    # Get the indices of maximum element in numpy array\n",
    "    result = np.where(y_true_vec == np.amax(y_true_vec))\n",
    "    #print('Returned tuple of arrays :', result)\n",
    "    #print('List of Indices of maximum element :', result[0])\n",
    "    ##print(f\"[+][+] label = {pre_dict.get(int(result[0]))}\\n\")\n",
    "    key = result[0][0]\n",
    "\n",
    "    #LABEL = pre_dict.get(int(result[0]))\n",
    "    LABEL = pre_dict.get(int(key))\n",
    "    ##print(f\"1[+][+][+] label = {pre_dict2.get(LABEL)}\\n\")\n",
    "    return pre_dict2.get(LABEL)\n",
    "\n",
    "def prediction_processes(pred):\n",
    "    pred = pred[0]\n",
    "    pre_dict={0:\"Blouses\",1:\"Dresses\",2:\"Shorts\",3:\"Skirts\"}\n",
    "    pre_dict2 = {\"Blouses\": 'B', \"Dresses\": 'D', \"Shorts\": 'SH', \"Skirts\": 'SK'}\n",
    "    categorical = \"\"\"\n",
    "                Blouses_Shirts :  0\n",
    "                Dresses :  1\n",
    "                Shorts :  2\n",
    "                Skirts :  3\n",
    "            \"\"\"\n",
    "\n",
    "    #print(f\"pred len={len(pred)}\\n\")\n",
    "    #for i in range(len(pred)):\n",
    "        # print(f\"pred{i}={round(pred[i], 5)}\\n\")\n",
    "        #print(f\"pred{i}={pred[i]}\\n\")\n",
    "\n",
    "    # Get the maximum element from a Numpy array\n",
    "    maxElement = np.amax(pred)\n",
    "    #print('Max element from Numpy Array : ', maxElement)\n",
    "    ## Get the indices of maximum element in numpy array\n",
    "    result = np.where(pred == np.amax(pred))\n",
    "    #print('Returned tuple of arrays :', result)\n",
    "    #print('List of Indices of maximum element :', result[0])\n",
    "    key = result[0][0]\n",
    "    #print(f\"Res = {pre_dict.get(key)}\")\n",
    "    #LABEL = pre_dict.get(int(result[0]))\n",
    "    LABEL = pre_dict.get(int(key))\n",
    "    #print(f\"2[+][+][+] label = {pre_dict2.get(LABEL)}\\n\")\n",
    "    return pre_dict2.get(LABEL)\n",
    "\n",
    "\n",
    "\n",
    "def test_sklearn_metrics(y_true,y_pred):\n",
    "\n",
    "    #\n",
    "    # # Constants\n",
    "    # C = \"Cat\"\n",
    "    # F = \"Fish\"\n",
    "    # H = \"Hen\"\n",
    "    #print(y_true)\n",
    "    #print(y_pred)\n",
    "    # True values\n",
    "    #y_true = [C, C, C, C, C, C, F, F, F, F, F, F, F, F, F, F, H, H, H, H, H, H, H, H, H]\n",
    "    # Predicted values\n",
    "    #y_pred = [C, C, C, C, H, F, C, C, C, C, C, C, H, H, F, F, C, C, C, H, H, H, H, H, H]\n",
    "    # Print the confusion matrix\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    # Print the precision and recall, among other metrics\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "\n",
    "\n",
    "model_final = load_model()\n",
    "\n",
    "directory =r\"F:\\data for recommender system\\FINAL_DATA_SORT\\Test\"\n",
    "X= DataGenerator(directory,batch_size=2, shuffle=True, data_augmentation=False)\n",
    "\n",
    "###############################################3\n",
    "start = 0\n",
    "end = 1\n",
    "sample = len(X.X_path)\n",
    "print(\"Number of sample={}\\n\".format(sample))\n",
    "\n",
    "y_true = []\n",
    "y_predictions_list = []\n",
    "\n",
    "while end <= len(X.X_path):\n",
    "    path = X.X_path[start:end]\n",
    "    start = end\n",
    "    end = end + 1\n",
    "    batch_x, batch_y = X.data_generation(path)\n",
    "    batch_y = batch_y[0]\n",
    "    true_label_index = get_y_true_label(batch_y)\n",
    "\n",
    "    y_true.append(true_label_index)\n",
    "\n",
    "    #print(batch_x)\n",
    "    pred = model_final.predict(batch_x, verbose=0)\n",
    "    y_pred = prediction_processes(pred)\n",
    "\n",
    "    y_predictions_list.append(y_pred)\n",
    "\n",
    "test_sklearn_metrics(y_true,y_predictions_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
